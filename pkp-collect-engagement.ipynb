{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PKP crawler\n",
    "\n",
    "Collect altmetric data for PKP publications\n",
    "\n",
    "1. Collect FB shares from Altmetric.com via DOI\n",
    "2. Collect FB shares from FB directly via URLs\n",
    "    - Resolved DOI\n",
    "    - Original PKP URL\n",
    "    - (opt) PMID\n",
    "    - (opt) PMCID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import urllib\n",
    "from dateutil.parser import parse\n",
    "from random import shuffle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lxml.etree as ET\n",
    "from pathlib import Path\n",
    "import configparser\n",
    "from ATB.ATB.Facebook import Facebook\n",
    "from ATB.ATB.Altmetric import Altmetric\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm_notebook().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "Config = configparser.ConfigParser()\n",
    "Config.read('config.cnf')\n",
    "FACEBOOK_APP_ID = Config.get('facebook', 'app_id')\n",
    "FACEBOOK_APP_SECRET = Config.get('facebook', 'app_secret')\n",
    "ALTMETRIC_KEY = Config.get('altmetric', 'key')\n",
    "\n",
    "fb_graph = Facebook(app_id=FACEBOOK_APP_ID, app_secret=FACEBOOK_APP_SECRET)\n",
    "altmetric = Altmetric(api_key = ALTMETRIC_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path(\"output_files/PKP_rerun/\")\n",
    "input_file = Path(\"input_files/PKP_20171220.csv\")\n",
    "url_types = ['pkp', 'doi', 'pmc', 'pmid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions\n",
    "def load_dataset(ids_file, resolv_dois_file):\n",
    "    ncbi = pd.read_csv(ids_file, parse_dates=['ncbi_ts'], index_col=\"doi\")\n",
    "    resolved_dois = pd.read_csv(resolv_dois_file, parse_dates=['doi_resolve_ts'], index_col=\"doi\")\n",
    "    \n",
    "    df = ncbi.merge(resolved_dois[['doi_url']], left_index=True, right_index=True, how=\"inner\")\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "# Facebook\n",
    "def fb_query(url):\n",
    "    og_object = None\n",
    "    og_engagement = None\n",
    "    og_error = None\n",
    "    \n",
    "    try:\n",
    "        fb_response = fb_graph.get_object(\n",
    "            id=urllib.parse.quote_plus(url),\n",
    "            fields=\"engagement, og_object\"\n",
    "        )\n",
    "        \n",
    "        if 'og_object' in fb_response:\n",
    "            og_object = fb_response['og_object']\n",
    "        if 'engagement' in fb_response:\n",
    "            og_engagement = fb_response['engagement']\n",
    "    except Exception as e:\n",
    "        og_error = e\n",
    "  \n",
    "    return (og_object, og_engagement, og_error)\n",
    "\n",
    "def collect_fb_engagement(df):\n",
    "    out_df = df.copy()\n",
    "    for col in  ['og_obj', 'og_eng', 'og_err', 'ts']:\n",
    "        out_df[col] = None\n",
    "    \n",
    "    rows = list(df.itertuples())\n",
    "    for row in tqdm_notebook(rows, total=len(rows)):\n",
    "        now = datetime.datetime.now()\n",
    "        og_object, og_engagement, og_error = fb_query(row.url)\n",
    "        \n",
    "        if og_object:\n",
    "            out_df.loc[(out_df.doi==row.doi) & (out_df.type==row.type), 'og_obj'] = json.dumps(og_object)\n",
    "        if og_engagement:\n",
    "            out_df.loc[(out_df.doi==row.doi) & (out_df.type==row.type), 'og_eng'] = json.dumps(og_engagement)\n",
    "        if og_error:\n",
    "            out_df.loc[(out_df.doi==row.doi) & (out_df.type==row.type), 'og_err'] = str(og_error)\n",
    "        out_df.loc[(out_df.doi==row.doi) & (out_df.type==row.type), 'ts'] = str(now)\n",
    "        \n",
    "    return out_df\n",
    "        \n",
    "# Altmetric\n",
    "def collect_am_engagement(df):\n",
    "    result_cols = ['am_resp', 'am_err', 'ts']\n",
    "    out_df = pd.DataFrame(columns=result_cols, index=df.index)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    rows = list(df.itertuples())\n",
    "    for row in tqdm_notebook(rows, total=len(rows)):\n",
    "        try:\n",
    "            am_resp = altmetric.doi(doi=row.Index, fetch=True)\n",
    "            am_err = None\n",
    "        except Exception as e:\n",
    "            am_resp = None\n",
    "            am_err = e\n",
    "\n",
    "        out_df.loc[row.Index, 'am_resp'] = json.dumps(am_resp)\n",
    "        out_df.loc[row.Index, 'am_err'] = str(am_err)\n",
    "        out_df.loc[row.Index, 'ts'] = str(now)\n",
    "        \n",
    "    return out_df\n",
    "\n",
    "def extract_fb_shares(df):\n",
    "    result_cols = [x + \"_shares\" for x in url_types] + [x + \"_ogid\" for x in url_types]\n",
    "    shares = pd.DataFrame(columns=result_cols, index=df.doi.unique())\n",
    "    shares.index.name = \"doi\"\n",
    "    \n",
    "    rows = list(df[df.og_obj.notnull()].itertuples())\n",
    "    for row in tqdm_notebook(rows, total=len(rows)):\n",
    "        if row.type == \"pkp_url\":\n",
    "            shares.loc[row.doi, \"pkp_ogid\"] = str(json.loads(row.og_obj)['id'])\n",
    "            shares.loc[row.doi, \"pkp_shares\"] = float(json.loads(row.og_eng)['share_count'])\n",
    "        elif row.type == \"doi_url\":\n",
    "            shares.loc[row.doi, \"doi_ogid\"] = str(json.loads(row.og_obj)['id'])\n",
    "            shares.loc[row.doi, \"doi_shares\"] = float(json.loads(row.og_eng)['share_count'])        \n",
    "        elif row.type == \"pmid_url\":\n",
    "            shares.loc[row.doi, \"pmid_ogid\"] = str(json.loads(row.og_obj)['id'])\n",
    "            shares.loc[row.doi, \"pmid_shares\"] = float(json.loads(row.og_eng)['share_count'])     \n",
    "        elif row.type == \"pmc_url\":\n",
    "            shares.loc[row.doi, \"pmc_ogid\"] = str(json.loads(row.og_obj)['id'])\n",
    "            shares.loc[row.doi, \"pmc_shares\"] = float(json.loads(row.og_eng)['share_count'])\n",
    "    return shares\n",
    "\n",
    "def extract_am_shares(df):\n",
    "    result_cols = ['am_shares', 'am_id']\n",
    "    shares = pd.DataFrame(columns=result_cols, index=df.doi.unique())\n",
    "    shares.index.name = \"doi\"\n",
    "    \n",
    "    rows = list(df[df.am_resp.notnull()].itertuples())\n",
    "    for row in tqdm_notebook(rows, total=len(rows)):\n",
    "        if pd.notnull(row.am_resp):\n",
    "            try:\n",
    "                shares.loc[row.doi, \"am_id\"] = str(json.loads(row.am_resp)['altmetric_id'])\n",
    "            except:\n",
    "                shares.loc[row.doi, \"am_id\"] = None\n",
    "            try:\n",
    "                shares.loc[row.doi, \"am_shares\"] =  float(json.loads(row.am_resp)['counts']['facebook']['posts_count'])\n",
    "            except:\n",
    "                shares.loc[row.doi, \"am_shares\"] = 0.0\n",
    "        #if pd.notnull(row.og_eng):\n",
    "        #    shares.loc[row.doi, row.type.split(\"_\")[0]] =  int(json.loads(row.og_eng)['share_count'])\n",
    "        \n",
    "    return shares\n",
    "\n",
    "def remove_duplicate_og_ids(df):\n",
    "    \"\"\"\n",
    "    Some URLs have been assigned to the same Facebook OG IDs\n",
    "    Those share numbers should be investigated in more detail\n",
    "    but for now they are simply being overwritten with NaNs\n",
    "    \"\"\"\n",
    "    \n",
    "    ids = [x + \"_ogid\" for x in url_types]\n",
    "\n",
    "    for _ in ids:\n",
    "        bad_ones = df[(~df[_].isnull() & df.duplicated(subset=[_], keep=False))].index\n",
    "        df.loc[bad_ones, _.split(\"_\")[0]] = np.nan\n",
    "        print(\"Duplicate {} IDs: {}\".format(_, len(bad_ones)))\n",
    "    return df\n",
    "\n",
    "def get_identical_doi_pkp_urls(df):\n",
    "    \"\"\"\n",
    "    Sum of the individual FB shares (doi, pkp, pmc, pmid) but taking\n",
    "    into account that some URLs are identical for resolved DOI and\n",
    "    original PKP ones.\n",
    "\n",
    "    DOI == PKP: sum(pkp, pmc, pmid)\n",
    "    DOI != PKP: sum(pkp, doi, pmc, pmid)\n",
    "    \"\"\"\n",
    "    x = df[df.type.isin(['pkp_url', 'doi_url'])][['url', 'doi']].copy()\n",
    "    x['netloc'] = x.url.apply(lambda x: urllib.parse.urlparse(x).netloc)\n",
    "    x['path'] = x.url.apply(lambda x: urllib.parse.urlparse(x).path)\n",
    "    return x[x[['netloc', 'path']].duplicated()].doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_with_urls = pd.read_csv(data_folder / \"articles_with_urls.csv\", index_col=\"doi\")\n",
    "\n",
    "df_urls = article_with_urls.reset_index().melt(\n",
    "    value_vars=['pmc_url', 'pmid_url', 'pkp_url', 'doi_url'],\n",
    "    id_vars='doi',\n",
    "    value_name=\"url\",\n",
    "    var_name=\"type\")\n",
    "\n",
    "df_urls = df_urls.replace([\"None\", \"null\", \"\"], np.nan).dropna()\n",
    "df_urls = df_urls.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out invalid records\n",
    "\n",
    "- Different DOIs with identical PKP URLs\n",
    "- DOIs resolved to the same URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in [x + \"_url\" for x in url_types]:\n",
    "    selection = df_urls.copy()\n",
    "    selection = selection[selection.type == _]\n",
    "    selection['netloc'] = selection.url.apply(lambda x: urllib.parse.urlparse(x).netloc)\n",
    "    selection['path'] = selection.url.apply(lambda x: urllib.parse.urlparse(x).path)\n",
    "    bad_ones = selection[selection[['netloc', 'path']].duplicated(keep=False)].index\n",
    "    df_urls = df_urls.drop(bad_ones)\n",
    "    print(\"Removed {} rows with bad {}\".format(len(bad_ones), _))\n",
    "    \n",
    "# Sanity check: duplicate URLs need to appear in both PKP and DOI\n",
    "df_urls[df_urls.url.isin(df_urls[df_urls.url.duplicated()].url)].type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect FB engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fb_results = collect_fb_engagement(df_urls)\n",
    "fb_results.to_csv(data_folder / \"fb_responses.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Altmetric engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "am_results = collect_am_engagement(article_with_urls)\n",
    "am_results = am_results.replace(\"null\", np.nan)\n",
    "am_results.to_csv(data_folder / \"am_responses.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = pd.read_csv(input_file,\n",
    "                       index_col=\"doi\",\n",
    "                       parse_dates=[\"date\"]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fb_results = pd.read_csv(data_folder / \"fb_responses.csv\")\n",
    "am_results = pd.read_csv(data_folder / \"am_responses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_extracted = extract_fb_shares(fb_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_extracted = extract_am_shares(am_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_extracted = remove_duplicate_og_ids(fb_extracted)\n",
    "doi_pkp_identical = get_identical_doi_pkp_urls(fb_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_shares = fb_extracted[[x + \"_shares\" for x in url_types]].astype(float)\n",
    "am_shares = am_extracted[['am_shares']].astype(float)\n",
    "\n",
    "fb_shares['fb_shares'] = fb_shares[[x + \"_shares\" for x in url_types]].sum(axis=1)\n",
    "fb_shares.loc[doi_pkp_identical, 'fb_shares'] = fb_shares.loc[doi_pkp_identical, ['pkp_shares', 'pmc_shares', 'pmid_shares']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_shares.to_csv(data_folder / \"fb_shares.csv\")\n",
    "am_shares.to_csv(data_folder / \"am_shares.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altmetrics",
   "language": "python",
   "name": "altmetrics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
